{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error,mean_absolute_error, r2_score\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "    \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, lr, n_epochs, device, patience, lamda, alpha, model_name, \n",
    "                 gamma_rul_consistency=0.0, epsilon_drop=1.0): # <-- New parameters\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "            n_epochs (int): The number of training epoch\n",
    "            device: 'cuda' or 'cpu'\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            lamda (float): The weight of the main RUL loss\n",
    "            alpha (List[float]): The weights of Capacity (SOH) loss\n",
    "            model_name (str): The model save path\n",
    "            gamma_rul_consistency (float): Weight for the RUL sequential consistency loss.\n",
    "                                           Set to 0 to disable.\n",
    "            epsilon_drop (float): Minimum expected RUL drop for consistency loss.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.model_name = model_name\n",
    "        self.lamda = lamda\n",
    "        self.alpha = alpha\n",
    "        self.gamma_rul_consistency = gamma_rul_consistency # <-- Store new param\n",
    "        self.epsilon_drop = epsilon_drop                   # <-- Store new param\n",
    "\n",
    "    def train(self, train_loader, valid_loader, model, load_model):\n",
    "        model = model.to(self.device)\n",
    "        device = self.device\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        model_name = self.model_name\n",
    "        lamda = self.lamda\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        loss_fn = nn.MSELoss() # For main RUL and SOH losses\n",
    "        early_stopping = EarlyStopping(self.patience, verbose=True)\n",
    "        # loss_fn.to(self.device) # MSELoss doesn't need to be sent to device explicitly\n",
    "\n",
    "        train_loss_main_rul_metric = [] # To track the main RUL MSE for reporting\n",
    "        valid_loss_metric = [] # Validation metric (can be main RUL or combined SOH)\n",
    "        total_combined_loss_log = [] # To log the actual loss optimized\n",
    "        total_combined_loss_log_reg=[]\n",
    "        total_combined_loss_log_rul=[]\n",
    "        total_combined_loss_log_soh=[]\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            model.train()\n",
    "            y_true_rul_epoch, y_pred_rul_epoch = [], [] # For main RUL metric\n",
    "            batch_losses, batch_losses_reg,batch_losses_rul,batch_losses_soh = [],[],[],[] # Store combined loss from each batch\n",
    "\n",
    "            for step, (x, y) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device) # y[:,0] is true final RUL, y[:,1:] are 10 true SOHs\n",
    "\n",
    "                # --- MODIFIED SECTION FOR LOSS CALCULATION ---\n",
    "                # Ensure your model's forward pass is modified as discussed:\n",
    "                # It should return: pred_final_rul, pred_soh_sequence, pred_rul_sequence\n",
    "                pred_final_rul, pred_soh_sequence, pred_rul_sequence = model(x)\n",
    "\n",
    "                # 1. Main RUL Loss (on the single final RUL prediction)\n",
    "                # Ensure pred_final_rul is correctly shaped (B, 1) or (B,) for y[:,0] (B,)\n",
    "                loss_main_rul = lamda * loss_fn(pred_final_rul.squeeze(), y[:, 0])\n",
    "                \n",
    "                # 2. SOH Loss (on the sequence of 10 SOH predictions)\n",
    "                loss_main_soh = torch.tensor(0.0, device=device)\n",
    "                # y has shape (batch_size, 11) -> 1 RUL + 10 SOH\n",
    "                # So, y.shape[1] - 1 = 10 (number of SOH values)\n",
    "                num_soh_points = pred_soh_sequence.shape[1] # Should be 10\n",
    "                for i in range(num_soh_points):\n",
    "                    loss_main_soh += loss_fn(pred_soh_sequence[:, i], y[:, i + 1]) * alpha[i]\n",
    "                \n",
    "                # 3. RUL Sequential Consistency Loss (New)\n",
    "                loss_rul_consistency = torch.tensor(0.0, device=device)\n",
    "                if self.gamma_rul_consistency > 0 and pred_rul_sequence is not None:\n",
    "                    loss_rul_consistency = self.gamma_rul_consistency * \\\n",
    "                        compute_rul_sequential_consistency_loss(\n",
    "                            pred_rul_sequence, \n",
    "                            self.epsilon_drop, \n",
    "                            device\n",
    "                        )\n",
    "                \n",
    "                # Total loss for this batch\n",
    "                current_batch_total_loss = loss_main_rul + loss_main_soh + loss_rul_consistency\n",
    "                # --- END OF MODIFIED SECTION ---\n",
    "\n",
    "                current_batch_total_loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_losses.append(current_batch_total_loss.cpu().detach().numpy())\n",
    "                batch_losses_reg.append(loss_rul_consistency.cpu().detach().numpy())\n",
    "                batch_losses_rul.append(loss_main_rul.cpu().detach().numpy())\n",
    "                batch_losses_soh.append(loss_main_soh.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_rul_epoch.append(pred_final_rul.squeeze(-1)) # For main RUL training metric\n",
    "                y_true_rul_epoch.append(y[:, 0])\n",
    "\n",
    "            y_true_rul_epoch_cat = torch.cat(y_true_rul_epoch, axis=0)\n",
    "            y_pred_rul_epoch_cat = torch.cat(y_pred_rul_epoch, axis=0)\n",
    "\n",
    "            # For logging: main RUL MSE and average total optimized loss\n",
    "            epoch_main_rul_mse = mean_squared_error(\n",
    "                y_true_rul_epoch_cat.cpu().detach().numpy(), \n",
    "                y_pred_rul_epoch_cat.cpu().detach().numpy()\n",
    "            )\n",
    "            train_loss_main_rul_metric.append(epoch_main_rul_mse)\n",
    "            \n",
    "            avg_epoch_total_loss = np.mean(batch_losses)\n",
    "            avg_epoch_total_loss_reg = np.mean(batch_losses_reg)\n",
    "            avg_epoch_total_loss_rul = np.mean(batch_losses_rul)\n",
    "            avg_epoch_total_loss_soh = np.mean(batch_losses_soh)\n",
    "            \n",
    "            total_combined_loss_log.append(avg_epoch_total_loss)\n",
    "            total_combined_loss_log_reg.append(avg_epoch_total_loss_reg)\n",
    "            total_combined_loss_log_rul.append(avg_epoch_total_loss_rul)\n",
    "            total_combined_loss_log_soh.append(avg_epoch_total_loss_soh)\n",
    "            \n",
    "            # ---- Validation Part ----\n",
    "            model.eval()\n",
    "            y_true_val, y_pred_val = [], [] # For validation metric calculation\n",
    "            # The validation logic seems to calculate MSE based on different parts \n",
    "            # of the output depending on lambda and alpha.\n",
    "            # Early stopping should ideally monitor a consistent validation loss (e.g., sum of SOH MSEs or main RUL MSE).\n",
    "            # For simplicity, let's assume early stopping monitors the main RUL MSE on validation.\n",
    "            # You might need to adjust this if your primary validation metric is different.\n",
    "            \n",
    "            current_valid_losses_for_early_stopping = []\n",
    "            with torch.no_grad():\n",
    "                for step, (x_val, y_val) in enumerate(valid_loader):\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    \n",
    "                    # Assuming model returns all three parts\n",
    "                    val_final_rul, val_soh_seq, _ = model(x_val) # rul_sequence not used for this val loss\n",
    "                    \n",
    "                    # Example: Using main RUL MSE for validation metric\n",
    "                    # This needs to align with how early_stopping expects val_loss\n",
    "                    val_loss_batch = loss_fn(val_final_rul.squeeze(), y_val[:,0])\n",
    "                    current_valid_losses_for_early_stopping.append(val_loss_batch.cpu().numpy())\n",
    "\n",
    "            epoch_val_loss = np.mean(current_valid_losses_for_early_stopping)\n",
    "            valid_loss_metric.append(epoch_val_loss)\n",
    "            \n",
    "            if self.n_epochs > 100:\n",
    "                if (epoch % 100 == 0 and epoch !=0):\n",
    "                    print(f'Epoch: {epoch}')\n",
    "                    print(f'-- Train MainRUL MSE: {epoch_main_rul_mse:.4f}', \n",
    "                          f'-- Valid MainRUL MSE: {epoch_val_loss:.4f}',\n",
    "                          f'-- Avg Total Optimized Loss (Train): {avg_epoch_total_loss:.10f}',\n",
    "                         f'-- || Reg loss (Train): {avg_epoch_total_loss_reg:.10f}',\n",
    "                         f'-- || rul loss (Train): {avg_epoch_total_loss_rul:.10f}',\n",
    "                         f'-- || soh loss (Train): {avg_epoch_total_loss_soh:.10f}')\n",
    "\n",
    "                early_stopping(epoch_val_loss, model, f'{model_name}_best.pt') # Pass the chosen validation metric\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "                \n",
    "        if load_model:\n",
    "            print(f\"Loading best model from {model_name}_best.pt\")\n",
    "            model.load_state_dict(torch.load(f'{model_name}_best.pt'))\n",
    "        else:\n",
    "            #print(f\"Saving final model to {model_name}_end.pt\")\n",
    "            torch.save(model.state_dict(), f'{model_name}_end.pt')\n",
    "\n",
    "        # Return main RUL MSE log, validation metric log, and total optimized loss log\n",
    "        return model, train_loss_main_rul_metric, valid_loss_metric, total_combined_loss_log,total_combined_loss_log_reg,total_combined_loss_log_rul,total_combined_loss_log_soh\n",
    "\n",
    "    # def test(...): # Your test method seems okay, ensure it uses the modified model correctly.\n",
    "    # It will now get 3 outputs from model(x). Use pred_final_rul for y_ and pred_soh_sequence for soh_.\n",
    "    def test(self, test_loader, model):\n",
    "        model = model.to(self.device)\n",
    "        device = self.device\n",
    "\n",
    "        y_true_rul_list, y_pred_rul_list = [], []\n",
    "        soh_true_list, soh_pred_list = [], []\n",
    "        # If you want to inspect the RUL sequence during test:\n",
    "        # rul_sequence_list = [] \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (x_test, y_test) in enumerate(test_loader):\n",
    "                x_test = x_test.to(device)\n",
    "                y_test = y_test.to(device) # y_test[:,0] is RUL, y_test[:,1:] is SOH\n",
    "\n",
    "                # Model returns: pred_final_rul, pred_soh_sequence, pred_rul_sequence\n",
    "                pred_final_rul, pred_soh_sequence, _ = model(x_test) # pred_rul_sequence ignored for standard metrics\n",
    "                #print(pred_final_rul, pred_final_rul.shape)\n",
    "                #print(pred_final_rul.squeeze(-1),  pred_final_rul.squeeze(-1).shape)\n",
    "                y_pred_rul_list.append(pred_final_rul.squeeze(-1))\n",
    "                y_true_rul_list.append(y_test[:, 0])\n",
    "                \n",
    "                soh_pred_list.append(pred_soh_sequence)\n",
    "                soh_true_list.append(y_test[:, 1:])\n",
    "                # rul_sequence_list.append(pred_rul_sequence)\n",
    "\n",
    "        #print(len(y_pred_rul_list),y_pred_rul_list)\n",
    "        y_true_rul_cat = torch.cat(y_true_rul_list, axis=0)\n",
    "        y_pred_rul_cat = torch.cat(y_pred_rul_list, axis=0)\n",
    "        soh_true_cat = torch.cat(soh_true_list, axis=0)\n",
    "        soh_pred_cat = torch.cat(soh_pred_list, axis=0)\n",
    "        \n",
    "        mse_loss_rul = mean_squared_error(\n",
    "            y_true_rul_cat.cpu().detach().numpy(), \n",
    "            y_pred_rul_cat.cpu().detach().numpy()\n",
    "        )\n",
    "        # Note: Your original test returned SOH tensors directly.\n",
    "        # You can calculate SOH MSE similarly if needed.\n",
    "        return y_true_rul_cat, y_pred_rul_cat, mse_loss_rul, soh_true_cat, soh_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_rul_sequential_consistency_loss(rul_sequence_pred, epsilon_drop=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Computes the RUL sequential consistency loss.\n",
    "    Encourages RUL_t - RUL_{t+1} >= epsilon_drop.\n",
    "\n",
    "    Args:\n",
    "        rul_sequence_pred (torch.Tensor): Predicted RUL sequence for the window. \n",
    "                                          Shape: (batch_size, 10).\n",
    "        epsilon_drop (float): Minimum expected RUL decrease between consecutive steps.\n",
    "                              If the 10 cycles are consecutive battery cycles and RUL is \n",
    "                              in cycles, 1.0 is a good default. For just strict decrease,\n",
    "                              a very small positive number can be used.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar loss value (mean over batch and sequence).\n",
    "    \"\"\"\n",
    "    if rul_sequence_pred is None or rul_sequence_pred.shape[1] < 2:\n",
    "        # Not enough RUL predictions in the sequence to compute differences\n",
    "        return torch.tensor(0.0, device=device, requires_grad=False)\n",
    "\n",
    "    # Calculate actual drops: RUL_t+1 - RUL_{t} <0 ->RUL_{t+1}< RUL_t because there are less cycles\n",
    "    #from high values to low values so RUL_{t+1}< RUL_t \n",
    "    # RUL_t is rul_sequence_pred[:, :-1] (all but the last)\n",
    "    # RUL_{t+1} is rul_sequence_pred[:, 1:] (all but the first)\n",
    "    #reg\n",
    "    actual_drops = rul_sequence_pred[:, 1:] - rul_sequence_pred[:, :-1]  # Shape: (batch_size, 9)\n",
    "    #reg_fa (first all)\n",
    "    #last --first/2-- first\n",
    "    #actual_drops = rul_sequence_pred[:, -1] - rul_sequence_pred[:, 0]/2 #last>first>first/2 almost the same.\n",
    "    # Penalty occurs if actual_drops >0\n",
    "    loss_per_difference = F.relu(actual_drops)  # Shape: (batch_size, 9)\n",
    "\n",
    "    return loss_per_difference.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut, dropout):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nIn (int): The number of input unit\n",
    "            nHidden (int): The number of hidden unit\n",
    "            nOut (int): The number of output unit\n",
    "        \"\"\"\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=False, batch_first=True)\n",
    "        self.embedding = nn.Linear(nHidden, nOut)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        b, T, h = recurrent.size()\n",
    "        t_rec = recurrent.contiguous().view(b * T, h)\n",
    "        \n",
    "        if self.dropout:\n",
    "            t_rec = self.dropout(t_rec)\n",
    "        output = self.embedding(t_rec)\n",
    "        output = output.contiguous().view(b, T, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, ni, nc, no, nh, n_rnn=2, leakyRelu=False,sigmoid = False, mode=\"a_all\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ni (int): The number of input unit\n",
    "            nc (int): The number of original channel\n",
    "            no (int): The number of output unit\n",
    "            nh (int): The number of hidden unit\n",
    "\n",
    "        \"\"\"\n",
    "        super(CRNN, self).__init__()\n",
    "        self.mode=mode\n",
    "\n",
    "        ks = [3, 3, 3,    3, 3,   3, 3]\n",
    "        ps = [0, 0, 0,    0, 0,   0, 0]\n",
    "        ss = [2, 2, 2,    2, 2,   2, 1]\n",
    "        nm = [8, 16, 64,  64, 64, 64, 64]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, cnn, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            if i == 3: nIn = 64\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, (ks[i],1), (ss[i],1), (ps[i],0)))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0,cnn)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d((2,1), (2,1)))\n",
    "        convRelu(1,cnn)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d((2,1), (2,1)))\n",
    "        convRelu(2,cnn)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 1), (2, 1), (0, 0)))\n",
    "        self.sigmoid = sigmoid\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(64, nh, nh, False),\n",
    "            BidirectionalLSTM(nh, nh, no, False),)\n",
    "        #option 2 traditional:\n",
    "        if self.mode==\"rul_last\":\n",
    "            self.rul = nn.Linear(64, 1) \n",
    "            self.soh = nn.Linear(64, 1)\n",
    "        if self.mode in [\"a_all\",\"rul_all\"]:\n",
    "            self.rul = nn.Linear(10, 1) \n",
    "            self.soh = nn.Linear(64, 1)\n",
    "            \n",
    "        self.rul_sequential_head = nn.Linear(64, 1) # Assuming 'no' is the feature size per time step from RNN\n",
    "                #last a last loss \n",
    "        if self.mode==\"soh_a_l_last\":\n",
    "            self.soh = nn.Linear(64, 1)\n",
    "            self.rul = nn.Linear(10, 1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Input shape: [b, c, h, w]\n",
    "        Output shape: \n",
    "            rul [b, 1]\n",
    "            soh [b, 10]\n",
    "        \"\"\"\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(0, 2, 1)\n",
    "        output = self.rnn(conv)\n",
    "        #print(output.shape) #256,10,64\n",
    "        \n",
    "        # In CRNN forward method, after: output = self.rnn(conv)\n",
    "        # output shape: (batch_size, 10, num_rnn_features) e.g., (batch_size, 10, 64)\n",
    "        pred_rul_sequence = self.rul_sequential_head(output).squeeze(-1) # Shape: (batch_size, 10)\n",
    "        soh_pred_sequence = self.soh(output).squeeze(-1) # Assuming self.soh is Linear(num_rnn_features, 1)\n",
    "        \n",
    "\n",
    "        if self.mode in [\"rul_last\"]:\n",
    "            soh = output[:,-1,:].squeeze() \n",
    "            pred_final_rul=self.rul(soh)\n",
    "        if self.mode in [\"soh_a_l_last\"]:\n",
    "            soh = output[:,-1,:].squeeze() \n",
    "            soh = self.soh(soh)  # torch size [256, 1]\n",
    "            soh = soh.view(output.shape[0], 1)  # Asegura que sea [256, 1]\n",
    "            soh_pred_sequence = soh.repeat(1, output.shape[1])  # Repite en la segunda dimensi√≥n\n",
    "            \n",
    "        if self.mode not in [\"rul_last\"]:\n",
    "            pred_final_rul = self.rul(soh_pred_sequence) \n",
    "            \n",
    "\n",
    "        return pred_final_rul, soh_pred_sequence, pred_rul_sequence # New return signature        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' \n",
    "#model\n",
    "pretrain_model_path=f'model/wx_inner/Big_model_RUL_laststate_reg_0.1_best.pt' \n",
    "\n",
    "trainer = Trainer(lr = \"\", n_epochs = \"\",device = device, patience = \"\",\n",
    "                  lamda = \"\", alpha = \"\", model_name=\"\")\n",
    "\n",
    "tmp_fea, tmp_lbl = np.random.rand(13, 10, 100, 4),np.random.rand(13, 10)\n",
    "test_fea_ = tmp_fea[:].copy()\n",
    "test_lbl_ = tmp_lbl[:].copy()\n",
    "test_fea_ = test_fea_.transpose(0,3,2,1)\n",
    "testset = TensorDataset(torch.Tensor(test_fea_), torch.Tensor(test_lbl_))\n",
    "test_loader = DataLoader(testset,batch_size=len(testset), shuffle=False, drop_last = False)\n",
    "model = CRNN(100,4,64,64, mode=\"rul_last\") #this defines the forward pass, it depends on the model\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(pretrain_model_path)) #,map_location=torch.device('cpu')))\n",
    "y_true, y_pred, _, soh_true, soh_pred = trainer.test(test_loader, model)\n",
    "\n",
    "rul_pred=(y_pred.cpu().detach().numpy())\n",
    "soh_pred= soh_pred.cpu().detach().numpy()[:,-1] #last SoH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
